{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andrewjustin/anemoi-house-workflow/blob/master/colab-anemoi-workflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bb07c0c-6e0d-47b1-b1cd-ffe918a41c2d",
      "metadata": {
        "id": "7bb07c0c-6e0d-47b1-b1cd-ffe918a41c2d"
      },
      "source": [
        "# Anemoi Workflow Demo\n",
        "\n",
        "##### This notebook will guide you through the training an AI4NWP model with the Anemoi framework.\n",
        "\n",
        "##### For questions, please contact andrew.justin@noaa.gov."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd9d93ea-ffbb-4f4f-9fa7-423588461c90",
      "metadata": {
        "id": "dd9d93ea-ffbb-4f4f-9fa7-423588461c90"
      },
      "source": [
        "## 1) Environment Setup (10 minutes)\n",
        "\n",
        "**TODO**: Remove *ufs2arco* from environment and use wget to retrieve YAMLs and pre-generated dataset.\n",
        "\n",
        "**NOTE:** You will receive a popup after all packages are installed. Click \"**restart session**\" on the popup and continue on to the next step."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6333bcd8-1198-487d-bf89-9839c537c324",
      "metadata": {
        "scrolled": true,
        "id": "6333bcd8-1198-487d-bf89-9839c537c324",
        "outputId": "fc54f9ee-dba1-4629-d66e-b6f27d89d44e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting flash-attn\n",
            "  Using cached flash_attn-2.8.0.post2-cp311-cp311-linux_x86_64.whl\n",
            "Collecting torch (from flash-attn)\n",
            "  Using cached torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Collecting einops (from flash-attn)\n",
            "  Using cached einops-0.8.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting filelock (from torch->flash-attn)\n",
            "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting typing-extensions>=4.10.0 (from torch->flash-attn)\n",
            "  Using cached typing_extensions-4.14.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting sympy>=1.13.3 (from torch->flash-attn)\n",
            "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting networkx (from torch->flash-attn)\n",
            "  Using cached networkx-3.5-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting jinja2 (from torch->flash-attn)\n",
            "  Using cached jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting fsspec (from torch->flash-attn)\n",
            "  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch->flash-attn)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch->flash-attn)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch->flash-attn)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch->flash-attn)\n",
            "  Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch->flash-attn)\n",
            "  Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch->flash-attn)\n",
            "  Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.7.77 (from torch->flash-attn)\n",
            "  Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch->flash-attn)\n",
            "  Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch->flash-attn)\n",
            "  Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch->flash-attn)\n",
            "  Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch->flash-attn)\n",
            "  Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.6.77 (from torch->flash-attn)\n",
            "  Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch->flash-attn)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch->flash-attn)\n",
            "  Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.3.1 (from torch->flash-attn)\n",
            "  Using cached triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting setuptools>=40.8.0 (from triton==3.3.1->torch->flash-attn)\n",
            "  Using cached setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch->flash-attn)\n",
            "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->torch->flash-attn)\n",
            "  Using cached MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Using cached einops-0.8.1-py3-none-any.whl (64 kB)\n",
            "Using cached torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl (821.2 MB)\n",
            "Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
            "Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
            "Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
            "Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
            "Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
            "Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
            "Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
            "Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
            "Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
            "Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
            "Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
            "Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
            "Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
            "Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "Using cached triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
            "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "Using cached typing_extensions-4.14.0-py3-none-any.whl (43 kB)\n",
            "Using cached filelock-3.18.0-py3-none-any.whl (16 kB)\n",
            "Using cached fsspec-2025.5.1-py3-none-any.whl (199 kB)\n",
            "Using cached jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Using cached networkx-3.5-py3-none-any.whl (2.0 MB)\n",
            "Using cached MarkupSafe-3.0.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23 kB)\n",
            "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
            "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "Installing collected packages: nvidia-cusparselt-cu12, mpmath, typing-extensions, sympy, setuptools, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, fsspec, filelock, einops, triton, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, jinja2, nvidia-cusolver-cu12, torch, flash-attn\n",
            "  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "    Found existing installation: nvidia-cusparselt-cu12 0.6.3\n",
            "    Uninstalling nvidia-cusparselt-cu12-0.6.3:\n",
            "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.3\n",
            "  Attempting uninstall: mpmath\n",
            "    Found existing installation: mpmath 1.3.0\n",
            "    Uninstalling mpmath-1.3.0:\n",
            "      Successfully uninstalled mpmath-1.3.0\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.14.0\n",
            "    Uninstalling typing_extensions-4.14.0:\n",
            "      Successfully uninstalled typing_extensions-4.14.0\n",
            "  Attempting uninstall: sympy\n",
            "    Found existing installation: sympy 1.14.0\n",
            "    Uninstalling sympy-1.14.0:\n",
            "      Successfully uninstalled sympy-1.14.0\n",
            "  Attempting uninstall: setuptools\n",
            "    Found existing installation: setuptools 80.9.0\n",
            "    Uninstalling setuptools-80.9.0:\n",
            "      Successfully uninstalled setuptools-80.9.0\n",
            "  Attempting uninstall: nvidia-nvtx-cu12\n",
            "    Found existing installation: nvidia-nvtx-cu12 12.6.77\n",
            "    Uninstalling nvidia-nvtx-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-nvtx-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.6.85\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.6.85:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.6.85\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.26.2\n",
            "    Uninstalling nvidia-nccl-cu12-2.26.2:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.26.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufile-cu12\n",
            "    Found existing installation: nvidia-cufile-cu12 1.11.1.6\n",
            "    Uninstalling nvidia-cufile-cu12-1.11.1.6:\n",
            "      Successfully uninstalled nvidia-cufile-cu12-1.11.1.6\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.4.1\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.4.1:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.4.1\n",
            "  Attempting uninstall: networkx\n",
            "    Found existing installation: networkx 3.5\n",
            "    Uninstalling networkx-3.5:\n",
            "      Successfully uninstalled networkx-3.5\n",
            "  Attempting uninstall: MarkupSafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.5.1\n",
            "    Uninstalling fsspec-2025.5.1:\n",
            "      Successfully uninstalled fsspec-2025.5.1\n",
            "  Attempting uninstall: filelock\n",
            "    Found existing installation: filelock 3.18.0\n",
            "    Uninstalling filelock-3.18.0:\n",
            "      Successfully uninstalled filelock-3.18.0\n",
            "  Attempting uninstall: einops\n",
            "    Found existing installation: einops 0.8.1\n",
            "    Uninstalling einops-0.8.1:\n",
            "      Successfully uninstalled einops-0.8.1\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.3.1\n",
            "    Uninstalling triton-3.3.1:\n",
            "      Successfully uninstalled triton-3.3.1\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.5.1.17\n",
            "    Uninstalling nvidia-cudnn-cu12-9.5.1.17:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17\n",
            "  Attempting uninstall: jinja2\n",
            "    Found existing installation: Jinja2 3.1.6\n",
            "    Uninstalling Jinja2-3.1.6:\n",
            "      Successfully uninstalled Jinja2-3.1.6\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.7.1\n",
            "    Uninstalling torch-2.7.1:\n",
            "      Successfully uninstalled torch-2.7.1\n",
            "  Attempting uninstall: flash-attn\n",
            "    Found existing installation: flash_attn 2.8.0.post2\n",
            "    Uninstalling flash_attn-2.8.0.post2:\n",
            "      Successfully uninstalled flash_attn-2.8.0.post2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "pylibcudf-cu12 25.2.1 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 20.0.0 which is incompatible.\n",
            "ibis-framework 9.5.0 requires toolz<1,>=0.11, but you have toolz 1.0.0 which is incompatible.\n",
            "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.31.1 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.1 which is incompatible.\n",
            "dask-expr 1.1.21 requires dask==2024.12.1, but you have dask 2025.5.1 which is incompatible.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1 which is incompatible.\n",
            "google-cloud-aiplatform 1.97.0 requires google-cloud-storage<3.0.0,>=1.32.0, but you have google-cloud-storage 3.1.1 which is incompatible.\n",
            "bigframes 2.7.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\n",
            "langchain-core 0.3.65 requires packaging<25,>=23.2, but you have packaging 25.0 which is incompatible.\n",
            "rapids-dask-dependency 25.2.0 requires dask==2024.12.1, but you have dask 2025.5.1 which is incompatible.\n",
            "rapids-dask-dependency 25.2.0 requires distributed==2024.12.1, but you have distributed 2025.5.1 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 20.0.0 which is incompatible.\n",
            "cuml-cu12 25.2.1 requires numba<0.61.0a0,>=0.59.1, but you have numba 0.61.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-3.0.2 einops-0.8.1 filelock-3.18.0 flash-attn-2.8.0.post2 fsspec-2025.5.1 jinja2-3.1.6 mpmath-1.3.0 networkx-3.5 nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 setuptools-80.9.0 sympy-1.14.0 torch-2.7.1 triton-3.3.1 typing-extensions-4.14.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_distutils_hack",
                  "pkg_resources"
                ]
              },
              "id": "6fb0cea6661044868006f0af367d4217"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install ufs2arco==0.6 mpi4py anemoi-datasets==0.5.23 anemoi-graphs==0.5.2 anemoi-models==0.5.0 anemoi-training==0.4.0 anemoi-inference 'numpy<2.3' 'earthkit-data<0.14.0' --force-reinstall\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea034d3d-cf89-4601-b200-8d02fd328cbb",
      "metadata": {
        "id": "ea034d3d-cf89-4601-b200-8d02fd328cbb"
      },
      "source": [
        "## 2) Upload and Extract ZIP Folder containing YAML files\n",
        "\n",
        "- Upload the provided *anemoi.zip* folder to your current colab session, then run the cell below."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip anemoi.zip -d ."
      ],
      "metadata": {
        "id": "iDC9IcjiBTXO",
        "outputId": "5ea016c0-c6c2-4d9d-ae0b-69828e70329e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "iDC9IcjiBTXO",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  anemoi.zip\n",
            "   creating: ./data/\n",
            "  inflating: ./data/zarr.yaml        \n",
            "   creating: ./dataloader/\n",
            "  inflating: ./dataloader/native_grid.yaml  \n",
            "   creating: ./datamodule/\n",
            "  inflating: ./datamodule/single.yaml  \n",
            "   creating: ./diagnostics/\n",
            "   creating: ./diagnostics/benchmark_profiler/\n",
            "  inflating: ./diagnostics/benchmark_profiler/detailed.yaml  \n",
            "  inflating: ./diagnostics/benchmark_profiler/simple.yaml  \n",
            "   creating: ./diagnostics/callbacks/\n",
            "  inflating: ./diagnostics/callbacks/placeholder.yaml  \n",
            "  inflating: ./diagnostics/callbacks/pretraining.yaml  \n",
            "  inflating: ./diagnostics/callbacks/rollout_eval.yaml  \n",
            "  inflating: ./diagnostics/evaluation.yaml  \n",
            "   creating: ./diagnostics/plot/\n",
            "  inflating: ./diagnostics/plot/detailed.yaml  \n",
            "  inflating: ./diagnostics/plot/none.yaml  \n",
            "   creating: ./graph/\n",
            "  inflating: ./graph/encoder_decoder_only.yaml  \n",
            "  inflating: ./graph/multi_scale.yaml  \n",
            "   creating: ./hardware/\n",
            "  inflating: ./hardware/example.yaml  \n",
            "   creating: ./hardware/files/\n",
            "  inflating: ./hardware/files/example.yaml  \n",
            "   creating: ./hardware/paths/\n",
            "  inflating: ./hardware/paths/example.yaml  \n",
            "  inflating: ./hardware/slurm.yaml   \n",
            "   creating: ./model/\n",
            "  inflating: ./model/gnn.yaml        \n",
            "  inflating: ./model/graphtransformer.yaml  \n",
            "  inflating: ./model/transformer.yaml  \n",
            "   creating: ./training/\n",
            "  inflating: ./training/default.yaml  \n",
            "  inflating: ./model-config.yaml     \n",
            "  inflating: ./testing.yaml          \n",
            "  inflating: ./training.yaml         \n",
            "  inflating: ./validation.yaml       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Dataset Generation\n",
        "\n",
        "### 3.1) Define 'Recipe' YAML Paths\n",
        "\n",
        "Datasets are generated with *ufs2arco* by referencing 'recipes' that define the structures of your training, validation, and testing datasets."
      ],
      "metadata": {
        "id": "36UCMge2A5Zr"
      },
      "id": "36UCMge2A5Zr"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "863d73c4-89f5-4f78-9090-d0f7f835cccf",
      "metadata": {
        "id": "863d73c4-89f5-4f78-9090-d0f7f835cccf"
      },
      "outputs": [],
      "source": [
        "train_yaml_path = 'training.yaml'  # training YAML path\n",
        "valid_yaml_path = 'validation.yaml'  # validation YAML path\n",
        "test_yaml_path = 'testing.yaml'  # testing YAML path"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6fa3e3c4-bc91-46aa-b8e9-bc6648da22f0",
      "metadata": {
        "id": "6fa3e3c4-bc91-46aa-b8e9-bc6648da22f0"
      },
      "source": [
        "The sample datasets in this notebook will include data for the following timeframes at 3-hourly intervals:\n",
        "- **Training**: 0z 1 Jan 1994 - 21z 2 Jan 1994\n",
        "- **Validation**: 0z 3 Jan 1994 - 21z 4 Jan 1994\n",
        "- **Testing**: 0z 5 Jan 1994 - 21z 6 Jan 1994"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f457070d-3d90-4536-be19-a052da4f8f25",
      "metadata": {
        "id": "f457070d-3d90-4536-be19-a052da4f8f25"
      },
      "source": [
        "### 3.2) Create the Training Dataset (1-2 minutes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "76a274fc-d916-49c0-9728-4143129629b4",
      "metadata": {
        "id": "76a274fc-d916-49c0-9728-4143129629b4",
        "outputId": "510e2942-80c4-436f-a0a9-d10942d499f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/ufs2arco\", line 5, in <module>\n",
            "    from ufs2arco.cli import main\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ufs2arco/__init__.py\", line 7, in <module>\n",
            "    from .cice6dataset import CICE6Dataset\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ufs2arco/cice6dataset.py\", line 5, in <module>\n",
            "    import xarray as xr\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/xarray/__init__.py\", line 3, in <module>\n",
            "    from xarray import coders, groupers, testing, tutorial, ufuncs\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/xarray/coders.py\", line 6, in <module>\n",
            "    from xarray.coding.times import CFDatetimeCoder, CFTimedeltaCoder\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/xarray/coding/times.py\", line 12, in <module>\n",
            "    import pandas as pd\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/__init__.py\", line 49, in <module>\n",
            "    from pandas.core.api import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/api.py\", line 47, in <module>\n",
            "    from pandas.core.groupby import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/groupby/__init__.py\", line 1, in <module>\n",
            "    from pandas.core.groupby.generic import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/groupby/generic.py\", line 69, in <module>\n",
            "    from pandas.core.groupby import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/groupby/ops.py\", line 25, in <module>\n",
            "    import pandas._libs.groupby as libgroupby\n",
            "  File \"<frozen importlib._bootstrap>\", line 405, in parent\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!ufs2arco {train_yaml_path}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1838d26b-5d3b-44bb-bf72-c8c819042c29",
      "metadata": {
        "id": "1838d26b-5d3b-44bb-bf72-c8c819042c29"
      },
      "source": [
        "### 3.3) Create the Validation Dataset (1-2 minutes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "390e574c-6f2a-45be-b551-8321da6903db",
      "metadata": {
        "id": "390e574c-6f2a-45be-b551-8321da6903db",
        "outputId": "bf78c8b1-5815-474a-81fb-31cb0bdc5b15",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ufs2arco/sources/cloud_zarr.py:36: FutureWarning: In a future version of xarray decode_timedelta will default to False rather than None. To silence this warning, set decode_timedelta to True, False, or a 'CFTimedeltaCoder' instance.\n",
            "  xds = xr.open_zarr(\n"
          ]
        }
      ],
      "source": [
        "!ufs2arco {valid_yaml_path}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08251df3-4048-4478-90af-d771fe97af1e",
      "metadata": {
        "id": "08251df3-4048-4478-90af-d771fe97af1e"
      },
      "source": [
        "### 3.4) Create the Testing Dataset (1-2 minutes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "6450db87-6868-4d81-b44f-7cefc59ffd30",
      "metadata": {
        "id": "6450db87-6868-4d81-b44f-7cefc59ffd30",
        "outputId": "2fc228d6-4517-48ba-fc56-b963b7cbcf8f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/ufs2arco/sources/cloud_zarr.py:36: FutureWarning: In a future version of xarray decode_timedelta will default to False rather than None. To silence this warning, set decode_timedelta to True, False, or a 'CFTimedeltaCoder' instance.\n",
            "  xds = xr.open_zarr(\n"
          ]
        }
      ],
      "source": [
        "!ufs2arco {test_yaml_path}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Model Setup & Training"
      ],
      "metadata": {
        "id": "JMzu2Ldm8WNc"
      },
      "id": "JMzu2Ldm8WNc"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1) Environment Variables\n",
        "\n",
        "- Anemoi requires a \"base seed\" and a SLURM job ID.\n",
        "  - The base seed is used to initialize model weights. Changing the seed will result in different initial model parameters.\n",
        "  - The SLURM job ID is required, even if you are not on SLURM (just leave it as \"0\").\n",
        "- Hydra can be configured to output more complete tracebacks for debugging purposes.\n"
      ],
      "metadata": {
        "id": "uSfdW3Ub8oJr"
      },
      "id": "uSfdW3Ub8oJr"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "### Required ###\n",
        "os.environ[\"ANEMOI_BASE_SEED\"] = \"42\"\n",
        "os.environ[\"SLURM_JOB_ID\"] = \"0\"\n",
        "\n",
        "### Optional ###\n",
        "os.environ['HYDRA_FULL_ERROR'] = \"1\"  # for debugging"
      ],
      "metadata": {
        "id": "MkPPZrVt83ys"
      },
      "id": "MkPPZrVt83ys",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2) Train the Model"
      ],
      "metadata": {
        "id": "hO1xHwazDcdX"
      },
      "id": "hO1xHwazDcdX"
    },
    {
      "cell_type": "code",
      "source": [
        "!anemoi-training train --config-name=model-config.yaml"
      ],
      "metadata": {
        "id": "tP26kQJnDgX0",
        "outputId": "390d5ef0-5560-4134-b562-c41b84dcae26",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "tP26kQJnDgX0",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-06-24 18:48:13 INFO Running anemoi training command with overrides: ['--config-name=model-config.yaml']\n",
            "2025-06-24 18:48:20 INFO NumExpr defaulting to 2 threads.\n",
            "2025-06-24 18:48:23 INFO Prepending current user directory (/content) to the search path.\n",
            "2025-06-24 18:48:23 INFO Search path is now: [provider=anemoi-cwd-searchpath-plugin, path=/content, provider=hydra, path=pkg://hydra.conf, provider=main, path=pkg://anemoi.training/config]\n",
            "[2025-06-24 18:48:24,044][anemoi.training.train.train][INFO] - Config validated.\n",
            "[2025-06-24 18:48:24,044][anemoi.training.train.train][INFO] - Run id: 9439c3c1-0d21-4532-8d40-0217feb9d337\n",
            "[2025-06-24 18:48:24,045][anemoi.training.train.train][INFO] - Checkpoints path: p1/training-output/checkpoint/9439c3c1-0d21-4532-8d40-0217feb9d337\n",
            "[2025-06-24 18:48:24,045][anemoi.training.train.train][INFO] - Plots path: p1/training-output/plots/9439c3c1-0d21-4532-8d40-0217feb9d337\n",
            "[2025-06-24 18:48:24,854][anemoi.graphs.nodes.builders.from_file][INFO] - Reading the dataset from p1/dataset/training.zarr.\n",
            "[2025-06-24 18:48:35,265][anemoi.graphs.edges.builders.cutoff][INFO] - Using CutOff-Edges (with radius = 144.6 km) between data and hidden.\n",
            "/usr/local/lib/python3.11/dist-packages/anemoi/graphs/edges/builders/cutoff.py:186: UserWarning: The 'torch-cluster' library is not installed. Installing 'torch-cluster' can significantly improve performance for graph creation. You can install it using 'pip install torch-cluster'.\n",
            "  warnings.warn(\n",
            "[2025-06-24 18:48:35,618][anemoi.graphs.edges.builders.cutoff][INFO] - CutOffEdges is removing 0 because they exceed the maximum allowed number of neighbors (64) for each target node.\n",
            "[2025-06-24 18:48:35,786][anemoi.graphs.edges.builders.knn][INFO] - Using KNN-Edges (with 3 nearest neighbours) between hidden and data.\n",
            "/usr/local/lib/python3.11/dist-packages/anemoi/graphs/edges/builders/knn.py:117: UserWarning: The 'torch-cluster' library is not installed. Installing 'torch-cluster' can significantly improve performance for graph creation. You can install it using 'pip install torch-cluster'.\n",
            "  warnings.warn(\n",
            "[2025-06-24 18:48:36,522][anemoi.graphs.create][INFO] - Cleaning graph.\n",
            "[2025-06-24 18:48:36,522][anemoi.graphs.create][INFO] - _grid_reference_distance deleted from graph.\n",
            "[2025-06-24 18:48:36,522][anemoi.graphs.create][INFO] - _dataset deleted from graph.\n",
            "[2025-06-24 18:48:36,522][anemoi.graphs.create][INFO] - _grid_reference_distance deleted from graph.\n",
            "[2025-06-24 18:48:36,523][anemoi.graphs.create][WARNING] - No output path specified. The graph will not be saved.\n",
            "[2025-06-24 18:48:36,556][anemoi.training.data.datamodule.singledatamodule][INFO] - Timeincrement set to 1 for data with frequency, 10800, and timestep, 10800\n",
            "[2025-06-24 18:48:36,563][anemoi.training.train.train][INFO] - Number of data variables: 95\n",
            "[2025-06-24 18:48:36,570][anemoi.training.train.train][INFO] - Training limits: max_epochs=3, max_steps=1000. Training will stop when either limit is reached first. Learning rate scheduler will run for 1000 steps.\n",
            "/usr/local/lib/python3.11/dist-packages/cudf/utils/_ptxcompiler.py:64: UserWarning: Error getting driver and runtime versions:\n",
            "\n",
            "stdout:\n",
            "\n",
            "\n",
            "\n",
            "stderr:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"<string>\", line 4, in <module>\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/numba_cuda/numba/cuda/cudadrv/driver.py\", line 314, in __getattr__\n",
            "    raise CudaSupportError(\"Error at driver init: \\n%s:\" %\n",
            "numba.cuda.cudadrv.error.CudaSupportError: Error at driver init: \n",
            "\n",
            "CUDA driver library cannot be found.\n",
            "If you are sure that a CUDA driver is installed,\n",
            "try setting environment variable NUMBA_CUDA_DRIVER\n",
            "with the file path of the CUDA driver shared library.\n",
            ":\n",
            "\n",
            "\n",
            "Not patching Numba\n",
            "  warnings.warn(msg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/cudf/utils/gpu_utils.py:62: UserWarning: Failed to dlopen libcuda.so.1\n",
            "  warnings.warn(str(e))\n",
            "[2025-06-24 18:48:40,278][anemoi.training.diagnostics.callbacks.plot][INFO] - Setting up asynchronous plotting ...\n",
            "[2025-06-24 18:48:40,282][anemoi.training.diagnostics.callbacks.plot][INFO] - Setting up asynchronous plotting ...\n",
            "[2025-06-24 18:48:40,291][anemoi.training.diagnostics.callbacks.plot][INFO] - Setting up asynchronous plotting ...\n",
            "[2025-06-24 18:48:40,291][anemoi.training.diagnostics.callbacks.plot][INFO] - Using defined accumulation colormap for fields: []\n",
            "[2025-06-24 18:48:40,293][anemoi.training.diagnostics.callbacks.plot][INFO] - Setting up asynchronous plotting ...\n",
            "[2025-06-24 18:48:40,295][anemoi.training.diagnostics.callbacks.plot][INFO] - Setting up asynchronous plotting ...\n",
            "[2025-06-24 18:48:40,296][anemoi.training.diagnostics.callbacks.plot][INFO] - Using precip histogram plotting method for fields: [].\n",
            "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:513: You passed `Trainer(accelerator='cpu', precision='16-mixed')` but AMP with fp16 is not supported on CPU. Using `precision='bf16-mixed'` instead.\n",
            "[2025-06-24 18:48:40,708][pytorch_lightning.utilities.rank_zero][INFO] - Using bfloat16 Automatic Mixed Precision (AMP)\n",
            "[2025-06-24 18:48:40,746][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: False, used: False\n",
            "[2025-06-24 18:48:40,746][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores\n",
            "[2025-06-24 18:48:40,746][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs\n",
            "[2025-06-24 18:48:40,815][lightning_fabric.utilities.seed][INFO] - Seed set to 42000\n",
            "/usr/local/lib/python3.11/dist-packages/anemoi/utils/provenance.py:148: UserWarning: The '__version__' attribute is deprecated and will be removed in MarkupSafe 3.1. Use feature detection, or `importlib.metadata.version(\"markupsafe\")`, instead.\n",
            "  versions[name] = str(module.__version__)\n",
            "[2025-06-24 18:48:43,969][anemoi.models.preprocessing.normalizer][INFO] - Normalizing: cos_julian_day is not normalized.\n",
            "[2025-06-24 18:48:43,969][anemoi.models.preprocessing.normalizer][INFO] - Normalizing: cos_local_time is not normalized.\n",
            "[2025-06-24 18:48:43,969][anemoi.models.preprocessing.normalizer][INFO] - Normalizing: cos_longitude is not normalized.\n",
            "[2025-06-24 18:48:43,969][anemoi.models.preprocessing.normalizer][INFO] - Normalizing: insolation is not normalized.\n",
            "[2025-06-24 18:48:43,970][anemoi.models.preprocessing.normalizer][INFO] - Normalizing: land_static is not normalized.\n",
            "[2025-06-24 18:48:43,970][anemoi.models.preprocessing.normalizer][INFO] - Normalizing: sin_julian_day is not normalized.\n",
            "[2025-06-24 18:48:43,970][anemoi.models.preprocessing.normalizer][INFO] - Normalizing: sin_latitude is not normalized.\n",
            "[2025-06-24 18:48:43,970][anemoi.models.preprocessing.normalizer][INFO] - Normalizing: sin_local_time is not normalized.\n",
            "[2025-06-24 18:48:43,970][anemoi.models.preprocessing.normalizer][INFO] - Normalizing: sin_longitude is not normalized.\n",
            "[2025-06-24 18:48:44,023][anemoi.models.layers.utils][INFO] - Linear kernel: DotDict({'_target_': 'torch.nn.Linear', '_partial_': True})\n",
            "[2025-06-24 18:48:44,024][anemoi.models.layers.utils][INFO] - LayerNorm kernel: DotDict({'_target_': 'torch.nn.LayerNorm', '_partial_': True})\n",
            "[2025-06-24 18:48:44,025][anemoi.models.layers.utils][INFO] - QueryNorm kernel: {'_target_': 'anemoi.models.layers.normalization.AutocastLayerNorm', '_partial_': True, 'bias': False}\n",
            "[2025-06-24 18:48:44,026][anemoi.models.layers.utils][INFO] - KeyNorm kernel: {'_target_': 'anemoi.models.layers.normalization.AutocastLayerNorm', '_partial_': True, 'bias': False}\n",
            "[2025-06-24 18:48:44,026][anemoi.models.layers.utils][INFO] - Linear kernel: DotDict({'_target_': 'torch.nn.Linear', '_partial_': True})\n",
            "[2025-06-24 18:48:44,027][anemoi.models.layers.utils][INFO] - LayerNorm kernel: DotDict({'_target_': 'torch.nn.LayerNorm', '_partial_': True})\n",
            "[2025-06-24 18:48:44,028][anemoi.models.layers.utils][INFO] - QueryNorm kernel: {'_target_': 'anemoi.models.layers.normalization.AutocastLayerNorm', '_partial_': True, 'bias': False}\n",
            "[2025-06-24 18:48:44,028][anemoi.models.layers.utils][INFO] - KeyNorm kernel: {'_target_': 'anemoi.models.layers.normalization.AutocastLayerNorm', '_partial_': True, 'bias': False}\n",
            "[2025-06-24 18:48:44,029][anemoi.models.layers.utils][INFO] - Linear kernel: DotDict({'_target_': 'torch.nn.Linear', '_partial_': True})\n",
            "[2025-06-24 18:48:44,030][anemoi.models.layers.utils][INFO] - LayerNorm kernel: DotDict({'_target_': 'torch.nn.LayerNorm', '_partial_': True})\n",
            "[2025-06-24 18:48:44,030][anemoi.models.layers.utils][INFO] - QueryNorm kernel: DotDict({'_target_': 'anemoi.models.layers.normalization.AutocastLayerNorm', '_partial_': True, 'bias': False})\n",
            "[2025-06-24 18:48:44,031][anemoi.models.layers.utils][INFO] - KeyNorm kernel: DotDict({'_target_': 'anemoi.models.layers.normalization.AutocastLayerNorm', '_partial_': True, 'bias': False})\n",
            "[2025-06-24 18:48:44,052][anemoi.models.layers.attention][INFO] - Using flash_attention\n",
            "[2025-06-24 18:48:44,105][anemoi.models.layers.attention][INFO] - Using flash_attention\n",
            "[2025-06-24 18:48:44,106][anemoi.models.layers.attention][INFO] - Using flash_attention\n",
            "[2025-06-24 18:48:44,106][anemoi.models.layers.attention][INFO] - Using flash_attention\n",
            "[2025-06-24 18:48:44,107][anemoi.models.layers.attention][INFO] - Using flash_attention\n",
            "[2025-06-24 18:48:44,108][anemoi.models.layers.attention][INFO] - Using flash_attention\n",
            "[2025-06-24 18:48:44,109][anemoi.models.layers.attention][INFO] - Using flash_attention\n",
            "[2025-06-24 18:48:44,110][anemoi.models.layers.attention][INFO] - Using flash_attention\n",
            "[2025-06-24 18:48:44,199][anemoi.training.losses.nodeweights][INFO] - Loading node attribute area_weight from the graph\n",
            "[2025-06-24 18:48:44,280][anemoi.training.train.forecaster.forecaster][INFO] - Pressure level scaling: use scaler NoPressureLevelScaler with slope 0.0000 and minimum 1.00\n",
            "[2025-06-24 18:48:44,365][anemoi.training.train.train][INFO] - The following submodules will NOT be trained: []\n",
            "[2025-06-24 18:48:44,452][lightning_fabric.utilities.distributed][INFO] - Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "[2025-06-24 18:48:44,458][pytorch_lightning.utilities.rank_zero][INFO] - ----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=gloo\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "[2025-06-24 18:48:45,114][lightning_fabric.utilities.seed][INFO] - [rank: 0] Seed set to 42000\n",
            "[2025-06-24 18:48:45,119][pytorch_lightning.callbacks.model_summary][INFO] - \n",
            "  | Name    | Type                 | Params | Mode \n",
            "---------------------------------------------------------\n",
            "0 | model   | AnemoiModelInterface | 3.4 M  | train\n",
            "1 | loss    | WeightedMSELoss      | 0      | train\n",
            "2 | metrics | ModuleList           | 0      | train\n",
            "---------------------------------------------------------\n",
            "3.4 M     Trainable params\n",
            "0         Non-trainable params\n",
            "3.4 M     Total params\n",
            "13.423    Total estimated model params size (MB)\n",
            "155       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "Sanity Checking: |          | 0/? [00:00<?, ?it/s][2025-06-24 18:48:45,123][anemoi.training.data.datamodule.singledatamodule][WARNING] - Training end date 1994 is not before validation start date 1994.\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:665: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "[2025-06-24 18:48:45,222][anemoi.training.data.dataset.singledataset][INFO] - Worker 0 (pid 8361, global_rank 0, model comm group 0)  has low/high range 0 / 1\n",
            "[2025-06-24 18:48:45,231][anemoi.training.data.dataset.singledataset][INFO] - Worker 0 (validation, pid 8361, glob. rank 0, model comm group 0, group_rank 0, seed group id 0, base_seed 42000, sanity rnd 0.315034)\n",
            "[2025-06-24 18:48:45,274][anemoi.training.data.dataset.singledataset][INFO] - Worker 1 (pid 8363, global_rank 0, model comm group 0)  has low/high range 1 / 2\n",
            "[2025-06-24 18:48:45,300][anemoi.training.data.dataset.singledataset][INFO] - Worker 1 (validation, pid 8363, glob. rank 0, model comm group 0, group_rank 0, seed group id 0, base_seed 42000, sanity rnd 0.315034)\n",
            "[2025-06-24 18:48:45,362][anemoi.training.data.dataset.singledataset][INFO] - Worker 2 (pid 8365, global_rank 0, model comm group 0)  has low/high range 2 / 3\n",
            "[2025-06-24 18:48:45,374][anemoi.training.data.dataset.singledataset][INFO] - Worker 2 (validation, pid 8365, glob. rank 0, model comm group 0, group_rank 0, seed group id 0, base_seed 42000, sanity rnd 0.315034)\n",
            "[2025-06-24 18:48:45,406][anemoi.training.data.dataset.singledataset][INFO] - Worker 3 (pid 8367, global_rank 0, model comm group 0)  has low/high range 3 / 4\n",
            "[2025-06-24 18:48:45,410][anemoi.training.data.dataset.singledataset][INFO] - Worker 3 (validation, pid 8367, glob. rank 0, model comm group 0, group_rank 0, seed group id 0, base_seed 42000, sanity rnd 0.315034)\n",
            "[2025-06-24 18:48:45,476][anemoi.training.data.dataset.singledataset][INFO] - Worker 4 (pid 8369, global_rank 0, model comm group 0)  has low/high range 4 / 5\n",
            "[2025-06-24 18:48:45,480][anemoi.training.data.dataset.singledataset][INFO] - Worker 4 (validation, pid 8369, glob. rank 0, model comm group 0, group_rank 0, seed group id 0, base_seed 42000, sanity rnd 0.315034)\n",
            "[2025-06-24 18:48:45,513][anemoi.training.data.dataset.singledataset][INFO] - Worker 5 (pid 8371, global_rank 0, model comm group 0)  has low/high range 5 / 6\n",
            "[2025-06-24 18:48:45,519][anemoi.training.data.dataset.singledataset][INFO] - Worker 5 (validation, pid 8371, glob. rank 0, model comm group 0, group_rank 0, seed group id 0, base_seed 42000, sanity rnd 0.315034)\n",
            "[2025-06-24 18:48:45,570][anemoi.training.data.dataset.singledataset][INFO] - Worker 6 (pid 8373, global_rank 0, model comm group 0)  has low/high range 6 / 7\n",
            "[2025-06-24 18:48:45,582][anemoi.training.data.dataset.singledataset][INFO] - Worker 6 (validation, pid 8373, glob. rank 0, model comm group 0, group_rank 0, seed group id 0, base_seed 42000, sanity rnd 0.315034)\n",
            "[2025-06-24 18:48:45,693][anemoi.training.data.dataset.singledataset][INFO] - Worker 7 (pid 8375, global_rank 0, model comm group 0)  has low/high range 7 / 8\n",
            "[2025-06-24 18:48:45,720][anemoi.training.data.dataset.singledataset][INFO] - Worker 7 (validation, pid 8375, glob. rank 0, model comm group 0, group_rank 0, seed group id 0, base_seed 42000, sanity rnd 0.315034)\n",
            "[2025-06-24 18:48:45,751][anemoi.training.diagnostics.callbacks.sanity][INFO] - The order of the variables in the model matches the order in the data.\n",
            "Sanity Checking DataLoader 0:   0% 0/6 [00:00<?, ?it/s]Error executing job with overrides: []\n",
            "[rank0]: Traceback (most recent call last):\n",
            "[rank0]:   File \"/usr/local/bin/anemoi-training\", line 8, in <module>\n",
            "[rank0]:     sys.exit(main())\n",
            "[rank0]:              ^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/anemoi/training/__main__.py\", line 23, in main\n",
            "[rank0]:     cli_main(__version__, __doc__, COMMANDS)\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/anemoi/utils/cli.py\", line 232, in cli_main\n",
            "[rank0]:     cmd.run(args, unknown)\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/anemoi/training/commands/train.py\", line 85, in run\n",
            "[rank0]:     main()\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/anemoi/training/commands/train.py\", line 96, in main\n",
            "[rank0]:     anemoi_train()\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/hydra/main.py\", line 94, in decorated_main\n",
            "[rank0]:     _run_hydra(\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/hydra/_internal/utils.py\", line 394, in _run_hydra\n",
            "[rank0]:     _run_app(\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/hydra/_internal/utils.py\", line 457, in _run_app\n",
            "[rank0]:     run_and_report(\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/hydra/_internal/utils.py\", line 223, in run_and_report\n",
            "[rank0]:     raise ex\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/hydra/_internal/utils.py\", line 220, in run_and_report\n",
            "[rank0]:     return func()\n",
            "[rank0]:            ^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/hydra/_internal/utils.py\", line 458, in <lambda>\n",
            "[rank0]:     lambda: hydra.run(\n",
            "[rank0]:             ^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/hydra/_internal/hydra.py\", line 132, in run\n",
            "[rank0]:     _ = ret.return_value\n",
            "[rank0]:         ^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/hydra/core/utils.py\", line 260, in return_value\n",
            "[rank0]:     raise self._return_value\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/hydra/core/utils.py\", line 186, in run_job\n",
            "[rank0]:     ret.return_value = task_function(task_cfg)\n",
            "[rank0]:                        ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/anemoi/training/train/train.py\", line 499, in main\n",
            "[rank0]:     AnemoiTrainer(config).train()\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/anemoi/training/train/train.py\", line 485, in train\n",
            "[rank0]:     trainer.fit(\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\", line 561, in fit\n",
            "[rank0]:     call._call_and_handle_interrupt(\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
            "[rank0]:     return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/strategies/launchers/subprocess_script.py\", line 105, in launch\n",
            "[rank0]:     return function(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\", line 599, in _fit_impl\n",
            "[rank0]:     self._run(model, ckpt_path=ckpt_path)\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1012, in _run\n",
            "[rank0]:     results = self._run_stage()\n",
            "[rank0]:               ^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1054, in _run_stage\n",
            "[rank0]:     self._run_sanity_check()\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1083, in _run_sanity_check\n",
            "[rank0]:     val_loop.run()\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
            "[rank0]:     return loop_run(self, *args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 145, in run\n",
            "[rank0]:     self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 437, in _evaluation_step\n",
            "[rank0]:     output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
            "[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/trainer/call.py\", line 328, in _call_strategy_hook\n",
            "[rank0]:     output = fn(*args, **kwargs)\n",
            "[rank0]:              ^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/strategies/strategy.py\", line 411, in validation_step\n",
            "[rank0]:     return self._forward_redirection(self.model, self.lightning_module, \"validation_step\", *args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/strategies/strategy.py\", line 641, in __call__\n",
            "[rank0]:     wrapper_output = wrapper_module(*args, **kwargs)\n",
            "[rank0]:                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
            "[rank0]:     return self._call_impl(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
            "[rank0]:     return forward_call(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/distributed.py\", line 1637, in forward\n",
            "[rank0]:     else self._run_ddp_forward(*inputs, **kwargs)\n",
            "[rank0]:          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/distributed.py\", line 1464, in _run_ddp_forward\n",
            "[rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
            "[rank0]:     return self._call_impl(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
            "[rank0]:     return forward_call(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/pytorch_lightning/strategies/strategy.py\", line 634, in wrapped_forward\n",
            "[rank0]:     out = method(*_args, **_kwargs)\n",
            "[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/anemoi/training/train/forecaster/forecaster.py\", line 691, in validation_step\n",
            "[rank0]:     val_loss, metrics, y_preds = self._step(batch, batch_idx, validation_mode=True)\n",
            "[rank0]:                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/anemoi/training/train/forecaster/forecaster.py\", line 514, in _step\n",
            "[rank0]:     for loss_next, metrics_next, y_preds_next in self.rollout_step(\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/anemoi/training/train/forecaster/forecaster.py\", line 484, in rollout_step\n",
            "[rank0]:     y_pred = self(x)\n",
            "[rank0]:              ^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
            "[rank0]:     return self._call_impl(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
            "[rank0]:     return forward_call(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/anemoi/training/train/forecaster/forecaster.py\", line 198, in forward\n",
            "[rank0]:     return self.model(x, self.model_comm_group)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
            "[rank0]:     return self._call_impl(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
            "[rank0]:     return forward_call(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/anemoi/models/models/encoder_processor_decoder.py\", line 298, in forward\n",
            "[rank0]:     x_latent_proc = self.processor(\n",
            "[rank0]:                     ^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
            "[rank0]:     return self._call_impl(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
            "[rank0]:     return forward_call(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/anemoi/models/layers/processor.py\", line 183, in forward\n",
            "[rank0]:     (x,) = self.run_layers((x,), shape_nodes, batch_size, model_comm_group, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/anemoi/models/layers/processor.py\", line 77, in run_layers\n",
            "[rank0]:     data = checkpoint(layer, *data, *args, **kwargs, use_reentrant=False)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/_compile.py\", line 51, in inner\n",
            "[rank0]:     return disable_fn(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/_dynamo/eval_frame.py\", line 838, in _fn\n",
            "[rank0]:     return fn(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/checkpoint.py\", line 495, in checkpoint\n",
            "[rank0]:     ret = function(*args, **kwargs)\n",
            "[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
            "[rank0]:     return self._call_impl(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
            "[rank0]:     return forward_call(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/anemoi/models/layers/chunk.py\", line 147, in forward\n",
            "[rank0]:     x = self.blocks[i](x, shapes, batch_size, model_comm_group=model_comm_group, **kwargs)\n",
            "[rank0]:         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
            "[rank0]:     return self._call_impl(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
            "[rank0]:     return forward_call(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/anemoi/models/layers/block.py\", line 119, in forward\n",
            "[rank0]:     x = x + self.attention(\n",
            "[rank0]:             ^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
            "[rank0]:     return self._call_impl(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
            "[rank0]:     return forward_call(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/anemoi/models/layers/attention.py\", line 165, in forward\n",
            "[rank0]:     out = self.attention(\n",
            "[rank0]:           ^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n",
            "[rank0]:     return self._call_impl(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n",
            "[rank0]:     return forward_call(*args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/anemoi/models/layers/attention.py\", line 278, in forward\n",
            "[rank0]:     out = self.attention(\n",
            "[rank0]:           ^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/flash_attn/flash_attn_interface.py\", line 1196, in flash_attn_func\n",
            "[rank0]:     return FlashAttnFunc.apply(\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/function.py\", line 575, in apply\n",
            "[rank0]:     return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/flash_attn/flash_attn_interface.py\", line 834, in forward\n",
            "[rank0]:     out_padded, softmax_lse, S_dmask, rng_state = _wrapped_flash_attn_forward(\n",
            "[rank0]:                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/_ops.py\", line 1158, in __call__\n",
            "[rank0]:     return self._op(*args, **(kwargs or {}))\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/_library/autograd.py\", line 113, in autograd_impl\n",
            "[rank0]:     result = forward_no_grad(*args, Metadata(keyset, keyword_only_args))\n",
            "[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/_library/autograd.py\", line 40, in forward_no_grad\n",
            "[rank0]:     result = op.redispatch(keyset & _C._after_autograd_keyset, *args, **kwargs)\n",
            "[rank0]:              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/_ops.py\", line 761, in redispatch\n",
            "[rank0]:     return self._handle.redispatch_boxed(keyset, *args, **kwargs)\n",
            "[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "[rank0]: NotImplementedError: Could not run 'flash_attn::_flash_attn_forward' with arguments from the 'CPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'flash_attn::_flash_attn_forward' is only available for these backends: [CUDA, Meta, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastMTIA, AutocastXPU, AutocastMPS, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n",
            "\n",
            "[rank0]: CUDA: registered at /dev/null:203 [kernel]\n",
            "[rank0]: Meta: registered at /dev/null:214 [kernel]\n",
            "[rank0]: BackendSelect: fallthrough registered at /pytorch/aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\n",
            "[rank0]: Python: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:194 [backend fallback]\n",
            "[rank0]: FuncTorchDynamicLayerBackMode: registered at /pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:479 [backend fallback]\n",
            "[rank0]: Functionalize: registered at /pytorch/aten/src/ATen/FunctionalizeFallbackKernel.cpp:349 [backend fallback]\n",
            "[rank0]: Named: registered at /pytorch/aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\n",
            "[rank0]: Conjugate: registered at /pytorch/aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\n",
            "[rank0]: Negative: registered at /pytorch/aten/src/ATen/native/NegateFallback.cpp:18 [backend fallback]\n",
            "[rank0]: ZeroTensor: registered at /pytorch/aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\n",
            "[rank0]: ADInplaceOrView: fallthrough registered at /pytorch/aten/src/ATen/core/VariableFallbackKernel.cpp:100 [backend fallback]\n",
            "[rank0]: AutogradOther: registered at /dev/null:203 [autograd kernel]\n",
            "[rank0]: AutogradCPU: registered at /dev/null:203 [autograd kernel]\n",
            "[rank0]: AutogradCUDA: registered at /dev/null:203 [autograd kernel]\n",
            "[rank0]: AutogradHIP: registered at /dev/null:203 [autograd kernel]\n",
            "[rank0]: AutogradXLA: registered at /dev/null:203 [autograd kernel]\n",
            "[rank0]: AutogradMPS: registered at /dev/null:203 [autograd kernel]\n",
            "[rank0]: AutogradIPU: registered at /dev/null:203 [autograd kernel]\n",
            "[rank0]: AutogradXPU: registered at /dev/null:203 [autograd kernel]\n",
            "[rank0]: AutogradHPU: registered at /dev/null:203 [autograd kernel]\n",
            "[rank0]: AutogradVE: registered at /dev/null:203 [autograd kernel]\n",
            "[rank0]: AutogradLazy: registered at /dev/null:203 [autograd kernel]\n",
            "[rank0]: AutogradMTIA: registered at /dev/null:203 [autograd kernel]\n",
            "[rank0]: AutogradPrivateUse1: registered at /dev/null:203 [autograd kernel]\n",
            "[rank0]: AutogradPrivateUse2: registered at /dev/null:203 [autograd kernel]\n",
            "[rank0]: AutogradPrivateUse3: registered at /dev/null:203 [autograd kernel]\n",
            "[rank0]: AutogradMeta: registered at /dev/null:203 [autograd kernel]\n",
            "[rank0]: AutogradNestedTensor: registered at /dev/null:203 [autograd kernel]\n",
            "[rank0]: Tracer: registered at /pytorch/torch/csrc/autograd/TraceTypeManual.cpp:294 [backend fallback]\n",
            "[rank0]: AutocastCPU: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:322 [backend fallback]\n",
            "[rank0]: AutocastMTIA: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:466 [backend fallback]\n",
            "[rank0]: AutocastXPU: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:504 [backend fallback]\n",
            "[rank0]: AutocastMPS: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:209 [backend fallback]\n",
            "[rank0]: AutocastCUDA: fallthrough registered at /pytorch/aten/src/ATen/autocast_mode.cpp:165 [backend fallback]\n",
            "[rank0]: FuncTorchBatched: registered at /pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:731 [backend fallback]\n",
            "[rank0]: BatchedNestedTensor: registered at /pytorch/aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:758 [backend fallback]\n",
            "[rank0]: FuncTorchVmapMode: fallthrough registered at /pytorch/aten/src/ATen/functorch/VmapModeRegistrations.cpp:27 [backend fallback]\n",
            "[rank0]: Batched: registered at /pytorch/aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\n",
            "[rank0]: VmapMode: fallthrough registered at /pytorch/aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\n",
            "[rank0]: FuncTorchGradWrapper: registered at /pytorch/aten/src/ATen/functorch/TensorWrapper.cpp:208 [backend fallback]\n",
            "[rank0]: PythonTLSSnapshot: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:202 [backend fallback]\n",
            "[rank0]: FuncTorchDynamicLayerFrontMode: registered at /pytorch/aten/src/ATen/functorch/DynamicLayer.cpp:475 [backend fallback]\n",
            "[rank0]: PreDispatch: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:206 [backend fallback]\n",
            "[rank0]: PythonDispatcher: registered at /pytorch/aten/src/ATen/core/PythonFallbackKernel.cpp:198 [backend fallback]\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Anemoi Workflow (ipykernel)",
      "language": "python",
      "name": "ipykernel"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}